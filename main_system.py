import streamlit as st
import pandas as pd
import sqlite3
import datetime
import json
import io
import re
import hashlib
import altair as alt
import numpy as np

# --- 1. å¸¸é‡é…ç½® ---
DB_NAME = "fund_data.db"

PERCENT_COLUMNS = [
    "å¯¹åº”ç®¡ç†ç±»è´¹çŽ‡åˆè®¡", "ç®¡ç†è´¹", "æ‰˜ç®¡è´¹", "æœåŠ¡è´¹", "å…¶ä»–ï¼ˆå¦‚æœ‰ï¼‰", 
    "ä¸šç»©æŠ¥é…¬è®¡ææ¯”ä¾‹", "ç”³è´­è´¹çŽ‡", "èµŽå›žè´¹çŽ‡", "å·¨é¢èµŽå›žè®¤å®šæ¯”ä¾‹", 
    "è¡Œæ”¿æœåŠ¡è´¹", "é”€å”®æœåŠ¡è´¹", "æŠ•èµ„é¡¾é—®è´¹", "ç”³è´­è´¹", "èµŽå›žè´¹"
]

STANDARD_COLUMNS = [
    "äº§å“åç§°", "ç­–ç•¥", "äº§å“ç±»åž‹", "å¼€æ”¾æ—¥", "å¯ä¹°ä»½é¢ç±»åž‹", 
    "ç”³è´­è´¹", "èµŽå›žè´¹", "å¯¹åº”ç®¡ç†ç±»è´¹çŽ‡åˆè®¡", "ç®¡ç†è´¹", "æ‰˜ç®¡è´¹", 
    "æœåŠ¡è´¹", "å…¶ä»–ï¼ˆå¦‚æœ‰ï¼‰", "ä¸šç»©åŸºå‡†", "ä¸šç»©æŠ¥é…¬è®¡ææ¯”ä¾‹", 
    "ç®¡ç†äºº", "é£Žé™©è¯„çº§", "ç”³è´­èµ·ç‚¹", "æ‰˜ç®¡äºº", "é”å®šæœŸ", 
    "æŠ•èµ„ç»ç†", "ç”³è´­ç¡®è®¤æ—¥", "èµŽå›žç¡®è®¤æ—¥", "èµŽå›žå›žæ¬¾æ—¥æœŸ", "å…¬å¸å®žæŽ§äºº", "åŸºé‡‘å¤‡æ¡ˆç¼–å·"
]

MAPPING_LOGIC = {
    "äº§å“å…¨ç§°": "äº§å“åç§°", 
    "äº§å“ç±»åž‹": "äº§å“ç±»åž‹", 
    "ç­–ç•¥": "ç­–ç•¥",
    "ç®¡ç†äººåç§°": "ç®¡ç†äºº", 
    "äº§å“é£Žé™©ç±»åˆ«": "é£Žé™©è¯„çº§",
    "é¦–æ¬¡è®¤è´­/ç”³è´­èµ·ç‚¹ï¼ˆä¸å«è®¤/ç”³è´­è´¹ï¼‰": "ç”³è´­èµ·ç‚¹", 
    "å¼€æ”¾æ—¥": "å¼€æ”¾æ—¥",
    "å°é—­æœŸ": "é”å®šæœŸ", 
    "ç”³è´­è´¹çŽ‡": "ç”³è´­è´¹", 
    "èµŽå›žè´¹çŽ‡": "èµŽå›žè´¹",
    "å¯¹åº”ç®¡ç†ç±»è´¹çŽ‡åˆè®¡ï¼ˆç®¡ç†è´¹çŽ‡+æ‰˜ç®¡è´¹çŽ‡+æœåŠ¡è´¹åˆè®¡+å…¶ä»–å¦‚æœ‰ï¼‰": "å¯¹åº”ç®¡ç†ç±»è´¹çŽ‡åˆè®¡",
    "ç®¡ç†è´¹çŽ‡": "ç®¡ç†è´¹", 
    "æ‰˜ç®¡è´¹çŽ‡": "æ‰˜ç®¡è´¹", 
    "æœåŠ¡è´¹åˆè®¡": "æœåŠ¡è´¹",
    "å…¶ä»–å¦‚æœ‰":"å…¶ä»–ï¼ˆå¦‚æœ‰ï¼‰", 
    "ä¸šç»©åŸºå‡†": "ä¸šç»©åŸºå‡†",
    "ä¸šç»©æŠ¥é…¬è®¡ææ¯”ä¾‹": "ä¸šç»©æŠ¥é…¬è®¡ææ¯”ä¾‹", 
    "å¯ä¹°ä»½é¢ç±»åž‹": "å¯ä¹°ä»½é¢ç±»åž‹",
    "ç”³è´­ç¡®è®¤æ—¥": "ç”³è´­ç¡®è®¤æ—¥", 
    "èµŽå›žç¡®è®¤æ—¥":"èµŽå›žç¡®è®¤æ—¥",
    "èµŽå›žå›žæ¬¾æ—¥æœŸ": "èµŽå›žå›žæ¬¾æ—¥æœŸ",
    "æŠ•èµ„ç»ç†": "æŠ•èµ„ç»ç†", 
    "å…¬å¸å®žæŽ§äºº": "å…¬å¸å®žæŽ§äºº",
    "åŸºé‡‘å¤‡æ¡ˆç¼–å·": "åŸºé‡‘å¤‡æ¡ˆç¼–å·", 
    "æ‰˜ç®¡äºº": "æ‰˜ç®¡äºº"
}

# --- 2. æ ¸å¿ƒå·¥å…·å‡½æ•° ---
def hash_password(password):
    return hashlib.sha256(str(password).encode()).hexdigest()

def to_percent_str(val):
    if val is None or pd.isna(val) or str(val).strip() == "": return ""
    if "%" in str(val): return str(val).strip()
    try:
        num = float(val)
        res = "{:.10f}".format(num * 100).rstrip('0').rstrip('.')
        return f"{res}%" if res != "0" else "0%"
    except:
        return str(val)

def force_plain_str(val, is_percent=False):
    if val is None or pd.isna(val): return ""
    val_str = str(val).strip()
    if val_str.lower() in ['nan', 'none', '']: return ""
    if is_percent: return to_percent_str(val_str)
    if 'e' in val_str.lower():
        try: return "{:.10f}".format(float(val)).rstrip('0').rstrip('.')
        except: return val_str
    if val_str.endswith('.0'): return val_str[:-2]
    return val_str

def parse_to_float(val):
    if not val or str(val).lower() in ["none", "nan", ""]: return 0.0
    val_str = str(val)
    res = re.findall(r"[-+]?\d*\.\d+|\d+", val_str)
    if not res: return 0.0
    num = float(res[0])
    return num / 100 if "%" in val_str else num

# --- 3. æ•°æ®åº“ç®¡ç† ---
def get_db_conn():
    conn = sqlite3.connect(DB_NAME, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    return conn

def init_db():
    conn = get_db_conn()
    conn.execute('''CREATE TABLE IF NOT EXISTS product_info 
                  (id INTEGER PRIMARY KEY AUTOINCREMENT,
                   p_name TEXT UNIQUE, p_manager TEXT, p_strategy TEXT, p_risk TEXT,
                   p_all_data TEXT, p_update_time TEXT)''')
    conn.execute('''CREATE TABLE IF NOT EXISTS users 
                  (username TEXT PRIMARY KEY, password TEXT, role TEXT, created_at TEXT)''')
    conn.execute('''CREATE TABLE IF NOT EXISTS net_values 
                  (id INTEGER PRIMARY KEY AUTOINCREMENT,
                   product_id INTEGER,
                   nv_date TEXT,
                   unit_nv REAL,
                   cum_nv REAL,
                   UNIQUE(product_id, nv_date))''')
    
    cursor = conn.execute("SELECT count(*) FROM users")
    if cursor.fetchone()[0] == 0:
        conn.execute("INSERT INTO users VALUES (?, ?, ?, ?)", 
                     ("admin", hash_password("888888"), "admin", datetime.datetime.now().strftime("%Y-%m-%d")))
        conn.execute("INSERT INTO users VALUES (?, ?, ?, ?)", 
                     ("staff", hash_password("123456"), "staff", datetime.datetime.now().strftime("%Y-%m-%d")))
    conn.commit()

def check_login(username, password):
    conn = get_db_conn()
    hashed_pw = hash_password(password)
    cursor = conn.execute("SELECT role FROM users WHERE username=? AND password=?", (username, hashed_pw))
    row = cursor.fetchone()
    return row['role'] if row else None

def add_user(username, password, role):
    conn = get_db_conn()
    try:
        conn.execute("INSERT INTO users VALUES (?, ?, ?, ?)", 
                     (username, hash_password(password), role, datetime.datetime.now().strftime("%Y-%m-%d")))
        conn.commit()
        return True, "æˆåŠŸ"
    except sqlite3.IntegrityError:
        return False, "ç”¨æˆ·åå·²å­˜åœ¨"
    except Exception as e:
        return False, str(e)

def delete_user(username):
    if username == 'admin': return False
    conn = get_db_conn()
    conn.execute("DELETE FROM users WHERE username=?", (username,))
    conn.commit()
    return True

def get_all_users():
    conn = get_db_conn()
    return pd.read_sql("SELECT username, role, created_at FROM users", conn)

def save_product_to_db(name, data_dict):
    conn = get_db_conn()
    clean_dict = {str(k): force_plain_str(v) for k, v in data_dict.items() if k}
    js = json.dumps(clean_dict, ensure_ascii=False)
    update_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    conn.execute('''INSERT OR REPLACE INTO product_info 
                 (p_name, p_manager, p_strategy, p_risk, p_all_data, p_update_time) 
                 VALUES (?,?,?,?,?,?)''', 
                 (str(name), clean_dict.get('ç®¡ç†äºº',''), clean_dict.get('ç­–ç•¥',''), 
                  clean_dict.get('é£Žé™©è¯„çº§',''), js, update_time))
    conn.commit()

def get_standard_dataframe(db_rows):
    flat_list = []
    for row in db_rows:
        try:
            raw_dict = json.loads(row['p_all_data'])
            record = {col: force_plain_str(raw_dict.get(col, ""), is_percent=(col in PERCENT_COLUMNS)) for col in STANDARD_COLUMNS}
            m, t, s, o = [parse_to_float(record.get(k, 0)) for k in ["ç®¡ç†è´¹", "æ‰˜ç®¡è´¹", "æœåŠ¡è´¹", "å…¶ä»–ï¼ˆå¦‚æœ‰ï¼‰"]]
            total = m + t + s + o
            record["å¯¹åº”ç®¡ç†ç±»è´¹çŽ‡åˆè®¡"] = force_plain_str(total, is_percent=True) if total > 0 else "0%"
            record["äº§å“åç§°"] = record.get("äº§å“åç§°") or row['p_name']
            flat_list.append(record)
        except: continue
    return pd.DataFrame(flat_list, columns=STANDARD_COLUMNS)

# --- 4. å‡€å€¼ä¸“ç”¨å‡½æ•° ---
def save_net_values(product_id, df):
    conn = get_db_conn()
    col_map = {}
    for c in df.columns:
        if "æ—¥æœŸ" in str(c) or "date" in str(c).lower(): col_map[c] = "nv_date"
        elif "å•ä½" in str(c): col_map[c] = "unit_nv"
        elif "ç´¯è®¡" in str(c): col_map[c] = "cum_nv"
    
    df = df.rename(columns=col_map)
    if "cum_nv" not in df.columns and "unit_nv" in df.columns:
        df["cum_nv"] = df["unit_nv"]
    
    if "nv_date" not in df.columns or "unit_nv" not in df.columns:
        return False, "ç¼ºå°‘'æ—¥æœŸ'æˆ–'å•ä½å‡€å€¼'åˆ—"

    count = 0
    for _, row in df.iterrows():
        try:
            d_str = pd.to_datetime(row['nv_date']).strftime("%Y-%m-%d")
            u_val = float(row['unit_nv'])
            c_val = float(row['cum_nv'])
            conn.execute("INSERT OR REPLACE INTO net_values (product_id, nv_date, unit_nv, cum_nv) VALUES (?,?,?,?)",
                         (product_id, d_str, u_val, c_val))
            count += 1
        except: continue
    conn.commit()
    return True, f"æˆåŠŸå¯¼å…¥ {count} æ¡å‡€å€¼"

def get_net_values_df(product_id):
    conn = get_db_conn()
    df = pd.read_sql("SELECT nv_date as 'æ—¥æœŸ', unit_nv as 'å•ä½å‡€å€¼', cum_nv as 'ç´¯è®¡å‡€å€¼' FROM net_values WHERE product_id=? ORDER BY nv_date ASC", conn, params=(product_id,))
    return df

# --- 5. ç•Œé¢ç»„ä»¶å°è£… ---

def ui_entry_tab():
    st.subheader("ðŸ“¤ å½•å…¥ä¸Žä¸Šä¼ ")
    
    # --- 1. åˆå§‹åŒ– ---
    if 'entry_df' not in st.session_state:
        st.session_state.entry_df = pd.DataFrame() 
    if 'processed_files' not in st.session_state:
        st.session_state.processed_files = []

    # --- 2. æ™ºèƒ½åˆ¤æ–­æ˜¯å¦ä¸ºç™¾åˆ†æ¯”åˆ— ---
    def is_percent_col(col_name):
        # 1. æ ¸å¿ƒå…³é”®è¯åŒ¹é… (åªè¦è¡¨å¤´å«è¿™äº›è¯ï¼Œå°±è‡ªåŠ¨è½¬ç™¾åˆ†æ¯”)
        keywords = ["è´¹çŽ‡", "æ¯”ä¾‹", "æ”¶ç›ŠçŽ‡", "æŠ˜æ‰£", "å æ¯”", "ç¨ŽçŽ‡", "è´¹", "ä¸šç»©æŠ¥é…¬"]
        if any(k in str(col_name) for k in keywords):
            return True
        # 2. ç”¨æˆ·ç‰¹æŒ‡çš„æ¨¡ç³Šè¯ (ä¾‹å¦‚ "å…¶ä»–å¦‚æœ‰")
        special_words = ["å…¶ä»–å¦‚æœ‰", "å…¶ä»–ï¼ˆå¦‚æœ‰ï¼‰"]
        if any(s in str(col_name) for s in special_words):
            return True
        # 3. åœ¨é¢„å®šä¹‰çš„æ ‡å‡†åˆ—è¡¨ä¸­
        return col_name in PERCENT_COLUMNS

    c1, c2, c3 = st.columns([2, 1, 1])
    with c1:
        uploaded_files = st.file_uploader("ä¸Šä¼ è¦ç´ è¡¨ (.xlsx)", type=["xlsx"], accept_multiple_files=True)
    with c2:
        st.write("")
        if st.button("âž• æ‰‹åŠ¨å¢žåŠ ä¸€è¡Œ", width='stretch'):
            current_cols = st.session_state.entry_df.columns.tolist()
            if not current_cols:
                current_cols = STANDARD_COLUMNS
            new_row = pd.DataFrame([{col: "" for col in current_cols}])
            new_row = new_row[current_cols]
            st.session_state.entry_df = pd.concat([st.session_state.entry_df, new_row], ignore_index=True)
            
    with c3:
        st.write("")
        if st.button("ðŸ§¹ æ¸…ç©ºåˆ—è¡¨", width='stretch'):
            st.session_state.entry_df = pd.DataFrame()
            st.session_state.processed_files = []
            st.rerun()

    if uploaded_files:
        new_data_list = []
        for f in uploaded_files:
            if f.name not in st.session_state.processed_files:
                try:
                    # è¯»å– Excel (ç»Ÿä¸€è¯»ä¸ºå­—ç¬¦ä¸²)
                    try:
                        raw_xlsx = pd.read_excel(f, header=None, dtype=str).fillna("")
                    except:
                        f.seek(0)
                        raw_xlsx = pd.read_excel(f, header=None, dtype=str).fillna("")

                    parsed = pd.DataFrame()
                    ordered_columns = []
                    
                    # --- A. ç«–ç‰ˆ/KVæ ¼å¼è§£æž ---
                    if "é¡¹ç›®" in str(raw_xlsx.iloc[0:15, 0].values):
                        keys = raw_xlsx[0].str.replace('*', '').str.replace('\n', '').str.strip()
                        val_col = 3 if raw_xlsx.shape[1] > 3 else raw_xlsx.shape[1]-1
                        
                        data_dict = {}
                        key_counter = {}
                        
                        for k, v in zip(keys, raw_xlsx[val_col]):
                            k_str = str(k).strip()
                            if k_str and k_str.lower() != 'nan' and k_str != "é¡¹ç›®":
                                # å¤„ç†é‡å¤é”® (æ·»åŠ  _2, _3)
                                if k_str in key_counter:
                                    key_counter[k_str] += 1
                                    unique_key = f"{k_str}_{key_counter[k_str]}"
                                else:
                                    key_counter[k_str] = 1
                                    unique_key = k_str
                                
                                # --- æ™ºèƒ½åˆ¤æ–­æ˜¯å¦åº”ç”¨ç™¾åˆ†æ¯”æ ¼å¼ ---
    
                                apply_percent = is_percent_col(k_str)
                                data_dict[unique_key] = force_plain_str(v, is_percent=apply_percent)
                                
                                ordered_columns.append(unique_key)
                        
                        parsed = pd.DataFrame([data_dict])
                        parsed = parsed[ordered_columns]
                        
                    # --- B. æ¨ªç‰ˆ/åˆ—è¡¨æ ¼å¼è§£æž ---
                    else:
                        f.seek(0)
                        parsed = pd.read_excel(f, dtype=str).fillna("").map(force_plain_str)

                        parsed = parsed.loc[:, ~parsed.columns.astype(str).str.contains('^Unnamed')]
                        for col in parsed.columns:
                            if is_percent_col(col):
                                parsed[col] = parsed[col].apply(lambda x: to_percent_str(x))
                        ordered_columns = parsed.columns.tolist()

                    # æ ‡å‡†åŒ–ä¸ŽæŽ’åºé€»è¾‘ (ä¿æŒä¸å˜)
                    for old_k, new_k in MAPPING_LOGIC.items():
                        if old_k in parsed.columns: parsed[new_k] = parsed[old_k]
                    
                    for col in STANDARD_COLUMNS:
                        if col not in parsed.columns: parsed[col] = ""
                    
                    final_order = []
                    seen = set()
                    for k in ordered_columns:
                        if k in parsed.columns and k not in seen:
                            final_order.append(k)
                            seen.add(k)
                    for k in parsed.columns:
                        if k not in seen:
                            final_order.append(k)
                            seen.add(k)      
                    parsed = parsed[final_order]

                    new_data_list.append(parsed)
                    st.session_state.processed_files.append(f.name)
                except Exception as e:
                    st.error(f"æ–‡ä»¶ {f.name} è¯»å–å¤±è´¥: {e}")

        if new_data_list:
            combined_new = pd.concat(new_data_list, ignore_index=True)
            if st.session_state.entry_df.empty:
                st.session_state.entry_df = combined_new
            else:
                st.session_state.entry_df = pd.concat([st.session_state.entry_df, combined_new], ignore_index=True).fillna("")
            st.success(f"æˆåŠŸå¯¼å…¥ {len(combined_new)} æ¡æ–°è®°å½•")
            st.rerun()

    if not st.session_state.entry_df.empty:
        st.info("ðŸ’¡ æç¤ºï¼šæ‰€æœ‰å«'è´¹'æˆ–'çŽ‡'çš„å­—æ®µå·²è‡ªåŠ¨è½¬ä¸ºç™¾åˆ†æ¯”ï¼›é‡å¤å­—æ®µå·²åœ¨æ˜¾ç¤ºæ—¶éšè—åŽç¼€ã€‚")
        
        # --- æ ¸å¿ƒä¿®å¤ï¼šæ›´å¼ºåŠ›çš„åˆ—é…ç½®ï¼Œéšè— _2, _3 ---
        my_column_config = {}
        for col_name in st.session_state.entry_df.columns:
            # åŒ¹é… _æ•°å­— ç»“å°¾çš„åˆ—å
            if re.search(r'_\d+$', str(col_name)):
                original_name = re.sub(r'_\d+$', '', str(col_name))
                # ä½¿ç”¨ TextColumn å¹¶æ˜Žç¡®æŒ‡å®š label
                my_column_config[col_name] = st.column_config.TextColumn(
                    label=original_name,
                    width="medium" 
                )
        
        edited_df = st.data_editor(
            st.session_state.entry_df, 
            num_rows="dynamic", 
            key="editor_main", 
            width='stretch',
            column_config=my_column_config # åº”ç”¨é…ç½®
        )
        
        if not edited_df.equals(st.session_state.entry_df):
            st.session_state.entry_df = edited_df

        if st.button("ðŸš€ ç¡®è®¤åŒæ­¥è‡³æ•°æ®åº“", width='stretch'):
            count = 0
            for _, row in edited_df.iterrows():
                name = row.get('äº§å“åç§°') or row.get('äº§å“å…¨ç§°')
                if name and str(name).strip() != "":
                    save_product_to_db(name, row.to_dict())
                    count += 1
            st.success(f"æˆåŠŸåŒæ­¥ {count} æ¡æ•°æ®")
            st.session_state.entry_df = pd.DataFrame()
            st.session_state.processed_files = [] 
            st.rerun()

def ui_card_edit_tab():
    """äº§å“å¡ç‰‡ç¼–è¾‘ (é›†æˆå‡€å€¼æ¨¡å— - å¢žå¼ºç‰ˆ)"""
    st.subheader("ðŸ” äº§å“å¡ç‰‡ç®¡ç†")
    conn = get_db_conn()
    db_df = pd.read_sql("SELECT * FROM product_info ORDER BY p_update_time DESC", conn)
    s_key = st.text_input("æœç´¢äº§å“åç§°...", placeholder="è¾“å…¥åç§°æˆ–ç­–ç•¥è¿›è¡Œè¿‡æ»¤")
    f_df = db_df[db_df['p_name'].str.contains(s_key)] if s_key else db_df

    if f_df.empty:
        st.caption("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³äº§å“")
    
    for _, r in f_df.iterrows():
        with st.expander(f"ðŸ“¦ {r['p_name']} (æ›´æ–°: {r['p_update_time'].split(' ')[0]})"):

            # --- åˆ†åŒº 1: å‡€å€¼ä¸Žèµ°åŠ¿ç®¡ç† (ç‹¬ç«‹ç›’å­) ---
            st.markdown("##### ðŸ“ˆ å‡€å€¼ä¸Žèµ°åŠ¿ç®¡ç†")
            
            with st.container(border=True):
                # 1.1 å‡€å€¼å¯¼å…¥åŒº
                c_imp, c_info = st.columns([1, 2])
                with c_imp:
                    nv_file = st.file_uploader(f"å¯¼å…¥å‡€å€¼åºåˆ— ({r['p_name']})", type=["xlsx", "csv"], key=f"nv_up_{r['id']}")
                    if nv_file:
                        try:
                            if nv_file.name.endswith('.csv'):
                                df_nv = pd.read_csv(nv_file)
                            else:
                                df_nv = pd.read_excel(nv_file)
                            
                            ok, msg = save_net_values(r['id'], df_nv)
                            if ok: st.success(msg)
                            else: st.error(msg)
                        except Exception as e:
                            st.error(f"è§£æžå¤±è´¥: {e}")
                
                with c_info:
                     st.info("æ”¯æŒ .xlsx/.csv æ ¼å¼ï¼Œéœ€åŒ…å«'æ—¥æœŸ'ã€'å•ä½å‡€å€¼'ã€'ç´¯è®¡å‡€å€¼'åˆ—ã€‚")

                # 1.2 å‡€å€¼å±•ç¤ºä¸Žåˆ†æžåŒº
                nv_df = get_net_values_df(r['id'])
                
                if not nv_df.empty:
                    # ç¡®ä¿æ—¥æœŸæ ¼å¼
                    nv_df["æ—¥æœŸ"] = pd.to_datetime(nv_df["æ—¥æœŸ"])
                    
                    # å¯¼èˆªæ  (Tabs)
                    tab_chart, tab_table = st.tabs(["ðŸ“Š æ”¶ç›Šèµ°åŠ¿åˆ†æž", "ðŸ“‹ åŽ†å²å‡€å€¼è¡¨"])
                    
                    with tab_chart:
                        # --- æ¢å¤ï¼šæœ€æ–°çŠ¶æ€å±•ç¤º (å§‹ç»ˆæ˜¾ç¤ºæœ€æ–°ä¸€æ¡æ•°æ®) ---
                        last_row = nv_df.iloc[-1]
                        st.caption("ðŸ”¹ æœ€æ–°çŠ¶æ€ (æˆªæ­¢æ•°æ®æœ«å°¾)")
                        m1, m2, m3 = st.columns(3)
                        m1.metric("æœ€æ–°å‡€å€¼æ—¥æœŸ", last_row['æ—¥æœŸ'].strftime('%Y-%m-%d'))
                        m2.metric("æœ€æ–°å•ä½å‡€å€¼", f"{last_row['å•ä½å‡€å€¼']:.4f}")
                        m3.metric("æœ€æ–°ç´¯è®¡å‡€å€¼", f"{last_row['ç´¯è®¡å‡€å€¼']:.4f}")
                        st.divider()

                        # --- A. åŒºé—´ç­›é€‰å™¨ (å®žçŽ°æ¨ªè½´ç¼©æ”¾ä¸ŽåŒºé—´è®¡ç®—) ---
                        min_date = nv_df["æ—¥æœŸ"].min().date()
                        max_date = nv_df["æ—¥æœŸ"].max().date()
                        
                        st.markdown("###### ðŸ“… åˆ†æžåŒºé—´é€‰æ‹©")
                        
                        # Fix: å¢žåŠ  key å‚æ•°ï¼Œé¿å… StreamlitDuplicateElementId é”™è¯¯
                        date_range = st.slider(
                            "æ‹–åŠ¨æ»‘å—é€‰æ‹©æ—¶é—´æ®µ",
                            min_value=min_date,
                            max_value=max_date,
                            value=(min_date, max_date),
                            format="YYYY-MM-DD",
                            label_visibility="collapsed",
                            key=f"date_slider_{r['id']}" 
                        )
                        
                        # æ ¹æ®æ»‘å—ç­›é€‰æ•°æ®
                        mask = (nv_df["æ—¥æœŸ"].dt.date >= date_range[0]) & (nv_df["æ—¥æœŸ"].dt.date <= date_range[1])
                        filtered_df = nv_df.loc[mask].sort_values("æ—¥æœŸ")

                        if len(filtered_df) > 1:
                            # --- B. æ ¸å¿ƒæŒ‡æ ‡è®¡ç®— ---
                            # 1. åŸºç¡€æ•°æ®å‡†å¤‡
                            start_nav = filtered_df["ç´¯è®¡å‡€å€¼"].iloc[0]
                            end_nav = filtered_df["ç´¯è®¡å‡€å€¼"].iloc[-1]
                            days_span = (filtered_df["æ—¥æœŸ"].iloc[-1] - filtered_df["æ—¥æœŸ"].iloc[0]).days
                            
                            # 2. æ”¶ç›ŠçŽ‡è®¡ç®—
                            interval_ret = (end_nav / start_nav) - 1 # åŒºé—´æ”¶ç›Š
                            
                            # å¹´åŒ–æ”¶ç›Š (å¤åˆ©å…¬å¼)
                            if days_span > 0:
                                annual_ret = (1 + interval_ret) ** (365 / days_span) - 1
                            else:
                                annual_ret = 0

                            # 3. æ³¢åŠ¨çŽ‡è®¡ç®— (è‡ªåŠ¨æŽ¨æ–­æ•°æ®é¢‘çŽ‡)
                            # è®¡ç®—å¹³å‡é—´éš”å¤©æ•°
                            if len(filtered_df) > 2:
                                avg_diff = filtered_df["æ—¥æœŸ"].diff().dt.days.mean()
                            else:
                                avg_diff = 1 # é»˜è®¤å€¼

                            if avg_diff <= 2: freq = 252       # æ—¥é¢‘
                            elif avg_diff <= 10: freq = 52     # å‘¨é¢‘
                            else: freq = 12                    # æœˆé¢‘
                            
                            pct_change = filtered_df["ç´¯è®¡å‡€å€¼"].pct_change().dropna()
                            volatility = pct_change.std() * np.sqrt(freq)

                            # 4. æœ€å¤§å›žæ’¤
                            # ç´¯è®¡æœ€å¤§å€¼åºåˆ—
                            roll_max = filtered_df["ç´¯è®¡å‡€å€¼"].cummax()
                            drawdown = (filtered_df["ç´¯è®¡å‡€å€¼"] - roll_max) / roll_max
                            max_drawdown = drawdown.min()

                            # 5. æ¯”çŽ‡è®¡ç®— (å‡è®¾æ— é£Žé™©åˆ©çŽ‡ä¸º 0)
                            sharpe = annual_ret / volatility if volatility != 0 and not pd.isna(volatility) else 0
                            calmar = annual_ret / abs(max_drawdown) if max_drawdown != 0 else 0

                            # --- C. æŒ‡æ ‡å±•ç¤º (ä¸¤è¡Œå¸ƒå±€) ---
                            st.caption(f"ðŸ”¹ åŒºé—´åˆ†æž ({date_range[0]} è‡³ {date_range[1]})")
                            k1, k2, k3, k4, k5, k6 = st.columns(6)
                            k1.metric("åŒºé—´æ”¶ç›Š", f"{interval_ret:.2%}", help="æœŸæœ«ç´¯è®¡å‡€å€¼ / æœŸåˆç´¯è®¡å‡€å€¼ - 1")
                            k2.metric("å¹´åŒ–æ”¶ç›ŠçŽ‡", f"{annual_ret:.2%}", help="((1+åŒºé—´æ”¶ç›Š)^(365/å¤©æ•°) - 1)")
                            k3.metric("å¹´åŒ–æ³¢åŠ¨çŽ‡", f"{volatility:.2%}", help=f"æ”¶ç›ŠçŽ‡æ ‡å‡†å·® * sqrt({freq})")
                            k4.metric("æœ€å¤§å›žæ’¤", f"{max_drawdown:.2%}", help="åŒºé—´å†…æœ€å¤§è·Œå¹…")
                            k5.metric("å¤æ™®æ¯”çŽ‡", f"{sharpe:.2f}", help="å¹´åŒ–æ”¶ç›Š / å¹´åŒ–æ³¢åŠ¨")
                            k6.metric("å¡çŽ›æ¯”çŽ‡", f"{calmar:.2f}", help="å¹´åŒ–æ”¶ç›Š / æœ€å¤§å›žæ’¤")
                            
                            st.divider()

                            # --- D. ç»˜å›¾ (Altair) ---
                            # çº¢è‰²æ¸å˜èƒŒæ™¯
                            gradient = alt.Gradient(
                                gradient='linear',
                                stops=[alt.GradientStop(color='rgba(255, 0, 0, 0.5)', offset=0), 
                                       alt.GradientStop(color='rgba(255, 255, 255, 0)', offset=1)],
                                x1=1, x2=1, y1=0, y2=1
                            )

                            base = alt.Chart(filtered_df).encode(
                                x=alt.X('æ—¥æœŸ:T', axis=alt.Axis(title=None, format='%Y-%m-%d'))
                            )

                            line = base.mark_line(color='#d62728', strokeWidth=3).encode(
                                y=alt.Y('ç´¯è®¡å‡€å€¼:Q', scale=alt.Scale(zero=False), axis=alt.Axis(title='ç´¯è®¡å‡€å€¼'))
                            )

                            area = base.mark_area(opacity=0.5).encode(
                                y='ç´¯è®¡å‡€å€¼:Q',
                                color=alt.value(gradient)
                            )
                            
                            # ç»„åˆå›¾è¡¨
                            chart = (area + line).properties(height=400).interactive()
                            st.altair_chart(chart, use_container_width=True)

                        else:
                            st.warning("æ‰€é€‰åŒºé—´æ•°æ®ä¸è¶³ï¼ˆè‡³å°‘éœ€è¦2ä¸ªæ•°æ®ç‚¹ï¼‰ï¼Œè¯·æ‰©å¤§é€‰æ‹©èŒƒå›´ã€‚")

                    with tab_table:
                        st.dataframe(nv_df, width='stretch', height=300)
                else:
                    st.info("æš‚æ— åŽ†å²å‡€å€¼æ•°æ®ï¼Œè¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼  Excel æˆ– CSV æ–‡ä»¶ã€‚")
            
            # --- åˆ†åŒº 2: åŸºç¡€è¦ç´ ç¼–è¾‘ ---
            st.markdown("##### ðŸ“ åŸºç¡€è¦ç´ ä¿¡æ¯")
            raw_data = json.loads(r['p_all_data'])
            display_df = pd.DataFrame([[k, force_plain_str(v, (k in PERCENT_COLUMNS))] for k, v in raw_data.items()], columns=["é¡¹", "å€¼"])
            new_details = st.data_editor(display_df, num_rows="dynamic", key=f"c_{r['id']}", width='stretch')
            
            c1, c2 = st.columns([1, 4])
            with c1:
                if st.button("ðŸ’¾ ä¿å­˜è¦ç´ ä¿®æ”¹", key=f"s_{r['id']}"):
                    save_product_to_db(r['p_name'], dict(new_details.values))
                    st.success("å·²æ›´æ–°"); st.rerun()
            with c2:
                if st.button("ðŸ—‘ï¸ åˆ é™¤äº§å“", key=f"d_{r['id']}"):
                    conn.execute("DELETE FROM product_info WHERE id=?", (r['id'],)); conn.commit(); st.rerun()
            
            st.divider()

def ui_standard_table_tab():
    """æ ‡å‡†è§†å›¾ (ç®¡ç†å‘˜)"""
    st.subheader("ðŸ“Š åœ¨åº“æ ‡å‡†è¡¨ (å…¨é‡ç®¡ç†)")
    conn = get_db_conn()
    rows = conn.execute("SELECT * FROM product_info").fetchall()
    std_df = get_standard_dataframe(rows)
    
    st.info("æç¤ºï¼šæ­¤è§†å›¾ä¸‹çš„ä¿®æ”¹ä¼šç›´æŽ¥è¦†ç›–æ•°æ®åº“ã€‚")
    edited_std = st.data_editor(std_df, num_rows="dynamic", width='stretch', key="std_admin")
    
    c1, c2 = st.columns([1, 1])
    with c1:
        if st.button("ðŸ“ æäº¤è¡¨æ ¼æ›´æ”¹", width='stretch'):
            for _, row in edited_std.iterrows():
                if row['äº§å“åç§°']: save_product_to_db(row['äº§å“åç§°'], row.to_dict())
            st.success("åŒæ­¥æˆåŠŸ"); st.rerun()
    with c2:
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            edited_std.to_excel(writer, index=False)
        st.download_button("ðŸ“¥ å¯¼å‡ºExcel", output.getvalue(), file_name=f"ç£æ¾æ•°æ®_{datetime.date.today()}.xlsx", width='stretch')

def ui_user_management_tab():
    """ç”¨æˆ·ç®¡ç† (ç®¡ç†å‘˜)"""
    st.subheader("ðŸ‘¥ å›¢é˜Ÿè´¦å·ç®¡ç†")
    users_df = get_all_users()
    st.dataframe(users_df, width='stretch') 
    st.divider()
    
    c1, c2 = st.columns(2)
    with c1:
        st.markdown("##### âž• æ–°å¢žæˆå‘˜")
        with st.form("add_user_form"):
            new_user = st.text_input("ç”¨æˆ·å")
            new_pass = st.text_input("åˆå§‹å¯†ç ", type="password")
            new_role = st.selectbox("è§’è‰²", ["staff", "admin"], help="admin: å…¨æƒ; staff: ä»…å½•å…¥/æŸ¥çœ‹")
            if st.form_submit_button("åˆ›å»º", width='stretch'):
                if new_user and new_pass:
                    ok, msg = add_user(new_user, new_pass, new_role)
                    if ok: st.success("åˆ›å»ºæˆåŠŸ"); st.rerun()
                    else: st.error(msg)
    with c2:
        st.markdown("##### âŒ åˆ é™¤æˆå‘˜")
        target = st.selectbox("é€‰æ‹©è´¦å·", users_df['username'].tolist())
        if st.button("åˆ é™¤è´¦å·", type="primary"):
            if target == 'admin': st.error("æ— æ³•åˆ é™¤è¶…çº§ç®¡ç†å‘˜")
            elif target == st.session_state.get('username'): st.error("ä¸èƒ½åˆ é™¤è‡ªå·±")
            else:
                delete_user(target)
                st.success("å·²åˆ é™¤"); st.rerun()