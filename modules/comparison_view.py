# modules/comparison_view.py
import streamlit as st
import pandas as pd
import numpy as np
import altair as alt
import datetime
from scipy import stats
from .database import get_db_conn

# --- 1. æ•°æ®è·å–ä¸å¤„ç† ---
def get_multi_product_data(product_ids):
    """æ ¹æ®å¤šä¸ªIDè·å–å‡€å€¼æ•°æ®ï¼Œå¹¶è½¬ä¸ºå®½è¡¨ (Index: Date, Cols: ProductName)"""
    if not product_ids:
        return pd.DataFrame()
    
    conn = get_db_conn()
    placeholders = ','.join(['?'] * len(product_ids))
    query = f"""
        SELECT t1.nv_date, t1.cum_nv, t2.p_name 
        FROM net_values t1
        JOIN product_info t2 ON t1.product_id = t2.id
        WHERE t1.product_id IN ({placeholders})
        ORDER BY t1.nv_date ASC
    """
    
    try:
        df = pd.read_sql(query, conn, params=tuple(product_ids))
        if df.empty:
            return pd.DataFrame()
        
        df['nv_date'] = pd.to_datetime(df['nv_date'])
        # é€è§†è¡¨ï¼šæ—¥æœŸä¸ºç´¢å¼•ï¼Œäº§å“åä¸ºåˆ—
        pivot_df = df.pivot(index='nv_date', columns='p_name', values='cum_nv')
        
        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘åªåšå‰å‘å¡«å……(å¤„ç†éäº¤æ˜“æ—¥)ï¼Œä½†ä¸ä½¿ç”¨dropna()ï¼Œä¿ç•™èµ·å§‹çš„ç©ºå€¼ä»¥ä¾¿"å…¨éƒ¨åŒºé—´"æ˜¾ç¤ºç©ºç™½
        pivot_df = pivot_df.ffill() 
        return pivot_df
    except Exception as e:
        st.error(f"æ•°æ®è¯»å–é”™è¯¯: {e}")
        return pd.DataFrame()

def calculate_financial_metrics(series, freq=252):
    """è®¡ç®—å•æ¡å‡€å€¼æ›²çº¿çš„æ ¸å¿ƒæŒ‡æ ‡ (è‡ªåŠ¨å»é™¤NaN)"""
    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘å…ˆå»é™¤ç©ºå€¼ï¼Œç¡®ä¿åªè®¡ç®—è¯¥äº§å“æœ‰æ•ˆå­˜ç»­æœŸçš„æ•°æ®
    clean_series = series.dropna()
    
    if len(clean_series) < 2:
        return {k: 0 for k in ["åŒºé—´æ”¶ç›Š", "å¹´åŒ–æ”¶ç›Š", "å¤æ™®æ¯”ç‡", "å¡ç›æ¯”ç‡", "æœ€å¤§å›æ’¤", "å¹´æ³¢åŠ¨ç‡", "æœ€å¤§å›æ’¤ä¿®å¤å¤©æ•°"]}

    # 1. åŸºç¡€æ•°æ®
    start_nav = clean_series.iloc[0]
    end_nav = clean_series.iloc[-1]
    days = (clean_series.index[-1] - clean_series.index[0]).days
    
    # 2. æ”¶ç›Šç‡
    interval_ret = (end_nav / start_nav) - 1
    if days > 0:
        annual_ret = (1 + interval_ret) ** (365 / days) - 1
    else:
        annual_ret = 0

    # 3. æ³¢åŠ¨ç‡ & å¤æ™®
    pct_change = clean_series.pct_change().fillna(0)
    volatility = pct_change.std() * np.sqrt(freq)
    risk_free = 0.00  # å‡è®¾æ— é£é™©åˆ©ç‡ 0%
    sharpe = (annual_ret - risk_free) / volatility if volatility != 0 else 0

    # 4. æœ€å¤§å›æ’¤ & ä¿®å¤å¤©æ•°
    roll_max = clean_series.cummax()
    drawdown = (clean_series - roll_max) / roll_max
    max_dd = drawdown.min()
    calmar = annual_ret / abs(max_dd) if max_dd != 0 else 0

# 5. æ¯”ç‡è®¡ç®— (å‡è®¾æ— é£é™©åˆ©ç‡ä¸º 0)
# sharpe = annual_ret / volatility if volatility != 0 and not pd.isna(volatility) else 0
# calmar = annual_ret / abs(max_drawdown) if max_drawdown != 0 else 0
#                            volatility = pct_change.std() * np.sqrt(freq)

    # ä¿®å¤å¤©æ•°è®¡ç®—
    repair_days = 0
    if max_dd < 0:
        idx_min = drawdown.idxmin()
        peak_val = roll_max.loc[idx_min]
        sub_series = clean_series.loc[idx_min:] 
        recover_points = sub_series[sub_series >= peak_val]
        if not recover_points.empty:
            repair_days = (recover_points.index[0] - idx_min).days
        else:
            repair_days = "æœªä¿®å¤" 

    return {
        "åŒºé—´æ”¶ç›Š": interval_ret,
        "å¹´åŒ–æ”¶ç›Š": annual_ret,
        "å¹´æ³¢åŠ¨ç‡": volatility,
        "æœ€å¤§å›æ’¤": max_dd,
        "å¤æ™®æ¯”ç‡": sharpe,
        "å¡ç›æ¯”ç‡": calmar,
        "æœ€å¤§å›æ’¤ä¿®å¤å¤©æ•°": repair_days
    }

def calculate_beta_alpha(target_series, benchmark_series, freq=252):
    """è®¡ç®—ç›¸å¯¹æŒ‡æ ‡ (Alpha/Beta)"""
    # å¯¹é½ç´¢å¼• (åªè®¡ç®—ä¸¤è€…éƒ½æœ‰æ•°æ®çš„æ—¥æœŸçš„äº¤é›†)
    common_idx = target_series.dropna().index.intersection(benchmark_series.dropna().index)
    
    if len(common_idx) < 10: # æ•°æ®å¤ªå°‘ä¸è®¡ç®—
        return 0, 0

    y = target_series.loc[common_idx].pct_change().fillna(0)
    x = benchmark_series.loc[common_idx].pct_change().fillna(0)
    
    # çº¿æ€§å›å½’
    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
    
    beta = slope
    alpha = intercept * freq 
    return beta, alpha

# --- 2. ç•Œé¢ä¸»å‡½æ•° ---
def ui_comparison_tab():
    st.subheader("âš”ï¸ äº§å“å¯¹æ¯”åˆ†æ")
    
    conn = get_db_conn()
    all_products = pd.read_sql("SELECT id, p_name FROM product_info", conn)
    
    # --- A. é¡¶éƒ¨æ§åˆ¶æ  ---
    with st.container(border=True):
        c1, c2 = st.columns([2, 1])
        with c1:
            selected_names = st.multiselect(
                "é€‰æ‹©å¯¹æ¯”äº§å“ (å»ºè®® 2-5 ä¸ª)", 
                options=all_products['p_name'].tolist(),
                default=all_products['p_name'].tolist()[:2] if len(all_products) >=2 else None
            )
        
        selected_ids = all_products[all_products['p_name'].isin(selected_names)]['id'].tolist()
        raw_df = get_multi_product_data(selected_ids)
        
        if raw_df.empty:
            st.info("è¯·é€‰æ‹©äº§å“ï¼Œæˆ–æ‰€é€‰äº§å“æš‚æ— å…±åŒæ—¶é—´æ®µçš„å‡€å€¼æ•°æ®ã€‚")
            return

        with c2:
            # --- æ ¸å¿ƒä¿®æ”¹ï¼šæ—¥æœŸé€»è¾‘è®¡ç®— ---
            # 1. å…¨éƒ¨åŒºé—´ (Union): å…¨å±€æœ€æ—©åˆ°å…¨å±€æœ€æ™š
            global_min = raw_df.index.min().date()
            global_max = raw_df.index.max().date()
            
            # 2. æœ€å¤§å…±åŒåŒºé—´ (Intersection): æ‰€æœ‰åˆ—éƒ½æœ‰å€¼çš„åŒºé—´çš„äº¤é›†
            # é€»è¾‘ï¼šæ‰¾åˆ°æ¯ä¸ªäº§å“çš„ç¬¬ä¸€å¤©ï¼Œå–æœ€å¤§å€¼ä½œä¸ºå…±åŒèµ·ç‚¹ï¼›æ‰¾åˆ°æ¯ä¸ªäº§å“çš„æœ€åä¸€å¤©ï¼Œå–æœ€å°å€¼ä½œä¸ºå…±åŒç»ˆç‚¹
            try:
                common_start = raw_df.apply(lambda x: x.first_valid_index()).max().date()
                common_end = raw_df.apply(lambda x: x.last_valid_index()).min().date()
            except:
                common_start, common_end = global_min, global_max # å®¹é”™å›é€€

            # 3. ä¸‹æ‹‰æ¡†é€‰é¡¹ (æœ€å¤§å…±åŒåŒºé—´æ’ç¬¬ä¸€ä½œä¸ºé»˜è®¤)
            time_range_opt = st.selectbox(
                "åˆ†ææ—¶æ®µ", 
                ["æœ€å¤§å…±åŒåŒºé—´", "å…¨éƒ¨åŒºé—´", "ä»Šå¹´ä»¥æ¥", "æœ€è¿‘ä¸€æœˆ", "æœ€è¿‘ä¸‰æœˆ", "æœ€è¿‘ä¸€å¹´", "è‡ªå®šä¹‰"]
            )
            
            start_date, end_date = common_start, common_end # é»˜è®¤å€¼
            today = datetime.date.today()
            
            if time_range_opt == "æœ€å¤§å…±åŒåŒºé—´":
                start_date, end_date = common_start, common_end
                if start_date > end_date:
                    st.warning("âš ï¸ é€‰ä¸­çš„äº§å“æ²¡æœ‰å…±åŒå­˜ç»­æ—¶é—´æ®µï¼Œå·²è‡ªåŠ¨åˆ‡æ¢ä¸ºå…¨éƒ¨åŒºé—´ã€‚")
                    start_date, end_date = global_min, global_max
            elif time_range_opt == "å…¨éƒ¨åŒºé—´":
                start_date, end_date = global_min, global_max
            elif time_range_opt == "ä»Šå¹´ä»¥æ¥":
                start_date = datetime.date(today.year, 1, 1)
                end_date = global_max
            elif time_range_opt == "æœ€è¿‘ä¸€æœˆ":
                start_date = today - datetime.timedelta(days=30)
                end_date = global_max
            elif time_range_opt == "æœ€è¿‘ä¸‰æœˆ":
                start_date = today - datetime.timedelta(days=90)
                end_date = global_max
            elif time_range_opt == "æœ€è¿‘ä¸€å¹´":
                start_date = today - datetime.timedelta(days=365)
                end_date = global_max
            elif time_range_opt == "è‡ªå®šä¹‰":
                d_range = st.date_input("é€‰æ‹©æ—¥æœŸèŒƒå›´", [common_start, common_end])
                if len(d_range) == 2:
                    start_date, end_date = d_range[0], d_range[1]

            # è¾¹ç•Œä¿®æ­£
            start_date = max(start_date, global_min)
            end_date = min(end_date, global_max)

    # --- æ•°æ®åˆ‡ç‰‡ ---
    mask = (raw_df.index.date >= start_date) & (raw_df.index.date <= end_date)
    sliced_df = raw_df.loc[mask]

    # å¦‚æœåˆ‡ç‰‡åå…¨æ˜¯ç©ºçš„ï¼ˆé’ˆå¯¹å…±åŒåŒºé—´æ²¡æ•°æ®çš„æƒ…å†µï¼‰
    if sliced_df.dropna(how='all').empty:
        st.warning("è¯¥æ—¶æ®µå†…æ— æœ‰æ•ˆæ•°æ®ã€‚")
        return

    # --- å½’ä¸€åŒ–æ•°æ® ---
    # ã€ä¿®æ”¹ç‚¹ã€‘å½’ä¸€åŒ–æ—¶ï¼Œå¦‚æœæŸäº§å“åœ¨èµ·ç‚¹æ˜¯NaNï¼Œå®ƒæ•´æ¡çº¿åº”è¯¥æ˜¯NaNï¼ˆæˆ–è€…æ˜¯ä»å®ƒæœ‰æ•°æ®çš„ç¬¬ä¸€å¤©å¼€å§‹å½’ä¸€åŒ–ï¼‰
    # è¿™é‡Œé‡‡ç”¨ç®€å•é€»è¾‘ï¼šæ¯åˆ—é™¤ä»¥è¯¥åˆ—åœ¨è¯¥åŒºé—´å†…ç¬¬ä¸€ä¸ªéç©ºå€¼
    normalized_df = sliced_df.copy()
    for col in normalized_df.columns:
        first_valid = normalized_df[col].first_valid_index()
        if first_valid:
            base_val = normalized_df.loc[first_valid, col]
            if base_val != 0:
                normalized_df[col] = normalized_df[col] / base_val

    # --- æ¨¡å— 1: æ”¶ç›Šç‡æ›²çº¿ ---
    st.markdown("##### 1. ğŸ“ˆ ç´¯è®¡æ”¶ç›Šç‡æ›²çº¿")
    
    chart_data = normalized_df.reset_index().melt('nv_date', var_name='äº§å“', value_name='ç´¯è®¡å‡€å€¼(å½’ä¸€åŒ–)')
    
    chart_yield = alt.Chart(chart_data).mark_line().encode(
        x=alt.X('nv_date:T', title=None, axis=alt.Axis(format='%Y-%m-%d')),
        y=alt.Y('ç´¯è®¡å‡€å€¼(å½’ä¸€åŒ–):Q', title='ç´¯è®¡æ”¶ç›Šè¶‹åŠ¿ (å„äº§å“èµ·ç‚¹=1)', scale=alt.Scale(zero=False)),
        color='äº§å“:N',
        tooltip=['nv_date', 'äº§å“', alt.Tooltip('ç´¯è®¡å‡€å€¼(å½’ä¸€åŒ–)', format='.4f')]
    ).properties(height=350).interactive()
    
    st.altair_chart(chart_yield, use_container_width=True)

    # --- æ¨¡å— 2: å›æ’¤èµ°åŠ¿ (æ ¸å¿ƒä¿®å¤ä½ç½®) ---
    st.markdown("##### 2. ğŸ“‰ åŠ¨æ€å›æ’¤åˆ†æ")
    
    # åŠ¨æ€å›æ’¤è®¡ç®—ï¼ˆéœ€å®¹å¿NaNï¼‰
    drawdown_df = sliced_df.copy()
    for col in drawdown_df.columns:
        # åªå¯¹éç©ºéƒ¨åˆ†è®¡ç®—å›æ’¤
        mask_valid = drawdown_df[col].notna()
        if mask_valid.any():
            roll_max = drawdown_df.loc[mask_valid, col].cummax()
            drawdown_df.loc[mask_valid, col] = (drawdown_df.loc[mask_valid, col] - roll_max) / roll_max

    # ä¿®å¤1: å¢åŠ  .dropna()ï¼Œè¿‡æ»¤æ‰æ²¡æœ‰æ•°æ®çš„è¡Œï¼Œè®©é¢ç§¯å›¾ä»çœŸæ­£æœ‰æ•°æ®é‚£å¤©å¼€å§‹æ¸²æŸ“
    chart_dd_data = drawdown_df.reset_index().melt('nv_date', var_name='äº§å“', value_name='å›æ’¤').dropna()
    
    # ä¿®å¤2: å¢åŠ  stack=Noneï¼Œé˜²æ­¢é¢ç§¯å›¾å †å å¯¼è‡´æ•°å€¼é”™è¯¯ç´¯åŠ 
    dd_area = alt.Chart(chart_dd_data).mark_area(opacity=0.3).encode(
        x=alt.X('nv_date:T', title=None),
        y=alt.Y('å›æ’¤:Q', axis=alt.Axis(format='%'), stack=None), # è¿™é‡Œå¢åŠ äº† stack=None
        y2=alt.value(0), 
        color='äº§å“:N'
    )

    dd_line = alt.Chart(chart_dd_data).mark_line(strokeWidth=1.5).encode(
        x=alt.X('nv_date:T'),
        y=alt.Y('å›æ’¤:Q'),
        color='äº§å“:N',
        tooltip=['nv_date', 'äº§å“', alt.Tooltip('å›æ’¤', format='.2%')]
    )
    
    st.altair_chart((dd_area + dd_line).properties(height=300).interactive(), use_container_width=True)

    # --- æ¨¡å— 3: æŒ‡æ ‡å¯¹æ¯”è¡¨æ ¼ ---
    st.markdown("##### 3. ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”")
    
    metrics_list = []
    # æ‰¾æ•°æ®æœ€å…¨çš„ä½œä¸ºåŸºå‡†ï¼Œæˆ–è€…é»˜è®¤ç¬¬ä¸€ä¸ª
    benchmark_col = sliced_df.columns[0] 
    benchmark_series = sliced_df[benchmark_col]

    for col in sliced_df.columns:
        series = sliced_df[col] # è¿™é‡Œå¯èƒ½åŒ…å«NaN
        m = calculate_financial_metrics(series)
        beta, alpha = calculate_beta_alpha(series, benchmark_series)
        
        row = {
            "äº§å“åç§°": col,
            "åŒºé—´æ”¶ç›Š": f"{m['åŒºé—´æ”¶ç›Š']:.2%}",
            "å¹´åŒ–æ”¶ç›Š": f"{m['å¹´åŒ–æ”¶ç›Š']:.2%}",
            "å¤æ™®æ¯”ç‡": f"{m['å¤æ™®æ¯”ç‡']:.2f}",
            "å¡ç›æ¯”ç‡": f"{m['å¡ç›æ¯”ç‡']:.2f}",
            "æœ€å¤§å›æ’¤": f"{m['æœ€å¤§å›æ’¤']:.2%}",
            "é˜¿å°”æ³•(Î±)": f"{alpha:.2%}", 
            "è´å¡”(Î²)": f"{beta:.2f}",    
            "å¹´æ³¢åŠ¨ç‡": f"{m['å¹´æ³¢åŠ¨ç‡']:.2%}",
            "ä¿®å¤å¤©æ•°": f"{m['æœ€å¤§å›æ’¤ä¿®å¤å¤©æ•°']} å¤©" if isinstance(m['æœ€å¤§å›æ’¤ä¿®å¤å¤©æ•°'], (int, float)) else m['æœ€å¤§å›æ’¤ä¿®å¤å¤©æ•°']
        }
        metrics_list.append(row)
    
    metrics_df = pd.DataFrame(metrics_list)
    cols_order = ["äº§å“åç§°", "åŒºé—´æ”¶ç›Š", "å¹´åŒ–æ”¶ç›Š", "æœ€å¤§å›æ’¤", "å¤æ™®æ¯”ç‡", "å¡ç›æ¯”ç‡", "é˜¿å°”æ³•(Î±)", "è´å¡”(Î²)", "å¹´æ³¢åŠ¨ç‡", "ä¿®å¤å¤©æ•°"]
    
    st.dataframe(metrics_df[cols_order], hide_index=True, use_container_width=True)
    st.caption(f"* æ³¨ï¼šç»Ÿè®¡æŒ‡æ ‡åŸºäºè¯¥æ—¶æ®µå†…å„äº§å“çš„ã€æœ‰æ•ˆå­˜ç»­æœŸã€‘è®¡ç®—ï¼›Alpha/Beta æš‚ä»¥ã€{benchmark_col}ã€‘ä¸ºåŸºå‡†ã€‚")

    # --- æ¨¡å— 4: ç›¸å…³æ€§çƒ­åŠ›å›¾ ---
    st.markdown("##### 4. ğŸ”— ç›¸å…³æ€§çŸ©é˜µ (çº¢é«˜ç»¿ä½)")
    
    # corr() è‡ªåŠ¨å¤„ç†NaN (Pairwise)
    corr_matrix = sliced_df.pct_change().corr().reset_index()
    corr_melt = corr_matrix.melt('p_name', var_name='äº§å“B', value_name='ç›¸å…³ç³»æ•°')
    
    base = alt.Chart(corr_melt).encode(
        x='p_name:O',
        y='äº§å“B:O'
    )

    heatmap = base.mark_rect().encode(
        color=alt.Color('ç›¸å…³ç³»æ•°:Q', 
                        scale=alt.Scale(scheme='redyellowgreen', domain=[-1, 1], reverse=True),
                        title="ç›¸å…³æ€§"),
        tooltip=['p_name', 'äº§å“B', alt.Tooltip('ç›¸å…³ç³»æ•°', format='.2f')]
    )

    text = base.mark_text(baseline='middle').encode(
        text=alt.Text('ç›¸å…³ç³»æ•°:Q', format='.2f'),
        color=alt.condition(
            alt.datum.ç›¸å…³ç³»æ•° > 0.5,
            alt.value('white'),
            alt.value('black')
        )
    )

    st.altair_chart((heatmap + text).properties(height=400), use_container_width=True)